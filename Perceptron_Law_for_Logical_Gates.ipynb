{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group6Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsvt2000/Neural_Networks_15CSE338/blob/main/Group6Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVs9srR9P-Br"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOqEBzrMNqr1"
      },
      "source": [
        "X = np.array([\n",
        "               [1,0,0],\n",
        "               [1,0,1],\n",
        "               [1,1,0],\n",
        "               [1,1,1] \n",
        "            ])\n",
        "X1 = np.array([\n",
        "               [1,0,0],\n",
        "               [1,0,1],\n",
        "               [1,1,0],\n",
        "               [1,1,1] \n",
        "            ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR8qeqC-OAiw"
      },
      "source": [
        "d = np.array([0,0,0,1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQSTkbV0OsMH"
      },
      "source": [
        "## Generalised Perceptron Learning Law"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUZFItUDQ8Ti"
      },
      "source": [
        "#training for 1 epoch\n",
        "def train_1_epoch(W,X,x0,x1):\n",
        "    # print(x0,x1)\n",
        "    x0 = x0.tolist()\n",
        "    x1 = x1.tolist()\n",
        "    for i in X:\n",
        "        i = i.tolist()\n",
        "        # print(W,i)\n",
        "        # print(np.dot(W,i))\n",
        "        if i in x0:\n",
        "            if np.dot(W,i)>=0:\n",
        "                # print(\"1a\")\n",
        "                W = W-i\n",
        "        elif i in x1:\n",
        "            if np.dot(W,i)<0:\n",
        "                # print(\"2a\")\n",
        "                W = W+i\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgL_1tMSUWdW"
      },
      "source": [
        "def train_perceptron(X,d):\n",
        "    #this works for the overall Logical gate\n",
        "    #Splitting positive and negative classes\n",
        "    x1 = []\n",
        "    x0 = []\n",
        "    for i in range(len(d)):\n",
        "        if(d[i] == 1):\n",
        "            x1.append(X[i])\n",
        "        else:\n",
        "            x0.append(X[i])\n",
        "    x1 = np.array(x1)\n",
        "    x0 = np.array(x0)\n",
        "    #print(x0,\"\\n\\n\",x1)\n",
        "    \n",
        "    #Assuming random weights\n",
        "    W = np.random.rand(3)\n",
        "    \n",
        "    flag = True\n",
        "    max_epochs = 100\n",
        "\n",
        "    #training until convergence or 100 epochs\n",
        "    while flag == True and max_epochs>0:\n",
        "        max_epochs -= 1\n",
        "        Wi = W\n",
        "        Wi1 = train_1_epoch(W,X,x0,x1)\n",
        "        delta_W = Wi==Wi1\n",
        "        # To check if Wi1 == Wi\n",
        "        if delta_W.all():\n",
        "            #print(True)\n",
        "            break\n",
        "        else:\n",
        "            W = Wi1\n",
        "        \n",
        "    # print(W)\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKVBA7_cI0u"
      },
      "source": [
        "## Generalized Perceptron Algorithm with learning rate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2TT3b7YcTu9"
      },
      "source": [
        "#training for 1 epoch\n",
        "def train_1_epoch_learn(W,X,x0,x1,e):\n",
        "    # print(x0,x1)\n",
        "    x0 = x0.tolist()\n",
        "    x1 = x1.tolist()\n",
        "    for i in X:\n",
        "        i = i.tolist()\n",
        "        # print(W,i)\n",
        "        # print(np.dot(W,i))\n",
        "        j=[k*e for k in i]\n",
        "        if i in x0:\n",
        "            if np.dot(W,i)>=0:\n",
        "                # print(\"1a\")\n",
        "                W = W-j\n",
        "        elif i in x1:\n",
        "            if np.dot(W,i)<0:\n",
        "                # print(\"2a\")\n",
        "                W = W+j\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jje-BaV6cdgQ"
      },
      "source": [
        "def train_perceptron_learn(X,d):\n",
        "    #this works for the overall Logical gate\n",
        "    #Splitting positive and negative classes\n",
        "    x1 = []\n",
        "    x0 = []\n",
        "    for i in range(len(d)):\n",
        "        if(d[i] == 1):\n",
        "            x1.append(X[i])\n",
        "        else:\n",
        "            x0.append(X[i])\n",
        "    x1 = np.array(x1)\n",
        "    x0 = np.array(x0)\n",
        "    #print(x0,\"\\n\\n\",x1)\n",
        "    \n",
        "    #Assuming random weights\n",
        "    W = np.random.rand(3)\n",
        "    e=random.uniform(0.0, 1.0)\n",
        "    print(\"Choosen learning rate: \",e)\n",
        "    flag = True\n",
        "    max_epochs = 100\n",
        "\n",
        "    #training until convergence or 100 epochs\n",
        "    while flag == True and max_epochs>0:\n",
        "        max_epochs -= 1\n",
        "        Wi = W\n",
        "        Wi1 = train_1_epoch_learn(W,X,x0,x1,e)\n",
        "        delta_W = Wi==Wi1\n",
        "        # To check if Wi1 == Wi\n",
        "        if delta_W.all():\n",
        "            #print(True)\n",
        "            break\n",
        "        else:\n",
        "            W = Wi1\n",
        "        \n",
        "    # print(W)\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxsGtNvNX5QJ"
      },
      "source": [
        "## Adjusted training set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5dFT-d-YFsv"
      },
      "source": [
        "def adjust_train_1_epoch(W,X):\n",
        "    # print(x0,x1)\n",
        "    for i in X:\n",
        "        #i = i.tolist()\n",
        "        # print(W,i)\n",
        "        #print(np.dot(W,i))\n",
        "        if np.dot(W,i)>0:\n",
        "          # print(\"2a\")\n",
        "          W = W-i\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFs1zluTX1Ud"
      },
      "source": [
        "def adjust_train_perceptron(X,d):\n",
        "    #Splitting positive and negative classes\n",
        "    X1=[]\n",
        "    for i in range(len(d)):\n",
        "        if(d[i] == 1):\n",
        "          X1.append(-X[i])\n",
        "          #X[i]=-X[i]\n",
        "        else:\n",
        "          X1.append(X[i])\n",
        "    #print(X)\n",
        "    X1=np.array(X1)\n",
        "    #Assuming random weights\n",
        "    W = np.random.rand(3)\n",
        "    \n",
        "    max_epochs = 100\n",
        "\n",
        "    #training until convergence or 100 epochs\n",
        "    while max_epochs>0:\n",
        "        max_epochs -= 1\n",
        "        Wi1 = adjust_train_1_epoch(W,X1)\n",
        "        f=0\n",
        "        for i in X1:\n",
        "          if np.dot(i,Wi1)<=0:\n",
        "            f=1\n",
        "            break\n",
        "        if f==0:\n",
        "          break\n",
        "        else:\n",
        "          W=Wi1          \n",
        "       \n",
        "    #print(max_epochs)\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwT-p420gK0V"
      },
      "source": [
        "## Adjusted Training Set with Learning Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3HGe5uPgbKh"
      },
      "source": [
        "def adjust_train_1_epoch_learn(W,X,e):\n",
        "    # print(x0,x1)\n",
        "    for i in X:\n",
        "        #i = i.tolist()\n",
        "        # print(W,i)\n",
        "        #print(np.dot(W,i))\n",
        "        j=[k*e for k in i]\n",
        "        if np.dot(W,j)>0:\n",
        "          # print(\"2a\")\n",
        "          W = W-j\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIchLHgogbKj"
      },
      "source": [
        "def adjust_train_perceptron_learn(X,d):\n",
        "    #Splitting positive and negative classes\n",
        "    X1=[]\n",
        "    for i in range(len(d)):\n",
        "        if(d[i] == 1):\n",
        "          X1.append(-X[i])\n",
        "          #X[i]=-X[i]\n",
        "        else:\n",
        "          X1.append(X[i])\n",
        "    #print(X)\n",
        "    X1=np.array(X1)\n",
        "    #Assuming random weights\n",
        "    W = np.random.rand(3)\n",
        "    e = random.uniform(0.0,1.0)\n",
        "    print(\"Choosen Learning rate: \",e)\n",
        "    max_epochs = 100\n",
        "\n",
        "    #training until convergence or 100 epochs\n",
        "    while max_epochs>0:\n",
        "        max_epochs -= 1\n",
        "        Wi1 = adjust_train_1_epoch(W,X1,e)\n",
        "        f=0\n",
        "        for i in X1:\n",
        "          if np.dot(i,Wi1)<=0:\n",
        "            f=1\n",
        "            break\n",
        "        if f==0:\n",
        "          break\n",
        "        else:\n",
        "          W=Wi1          \n",
        "       \n",
        "    #print(max_epochs)\n",
        "    return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ8t-LDoGpnI"
      },
      "source": [
        "## Verifying the perceptron for other logical gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NSRlMrBW4FV"
      },
      "source": [
        "def main():\n",
        "  ch='y'\n",
        "  while(ch=='y'):\n",
        "    c=int(input(\"1. AND function\\n2. OR function\\n3. NAND function\\n4. NOR function\\n5. IMPLIES function\\n6. NOT IMPLIES function\\n7. XOR function\\n8. XNOR function\"))\n",
        "    print()\n",
        "    if(c==1):\n",
        "      print(\"AND function\")\n",
        "      d = np.array([0,0,0,1])\n",
        "    elif(c==2):\n",
        "      print(\"OR function\")\n",
        "      d = np.array([0,1,1,1])\n",
        "    elif(c==3):\n",
        "      print(\"NAND function\")\n",
        "      d = np.array([1,1,1,0])\n",
        "    elif(c==5):\n",
        "      print(\"IMPLIES function\")\n",
        "      d = np.array([1,0,1,1])\n",
        "    elif(c==6):\n",
        "      print(\"NOT IMPLIES function\")\n",
        "      d = np.array([0,1,0,0])\n",
        "    elif (c==8):\n",
        "      print(\"XNOR function as a network\")\n",
        "      d1 = np.array([0,0,0,1])\n",
        "      W1 = train_perceptron(X,d1)\n",
        "      d2=np.array([1,0,0,0])\n",
        "      W2= train_perceptron(X,d2)\n",
        "      j=0\n",
        "      for i in X:\n",
        "        i[1]=d1[j]\n",
        "        i[2]=d2[j]\n",
        "        j+=1\n",
        "        if j==len(d1):\n",
        "          break\n",
        "      d=np.array([1,0,0,1])\n",
        "    elif (c==7):\n",
        "      print(\"XOR function as a network\")\n",
        "      d1 = np.array([0,0,0,1])\n",
        "      W1 = train_perceptron(X,d1)\n",
        "      d2=np.array([1,0,0,0])\n",
        "      W2= train_perceptron(X,d2)\n",
        "      j=0\n",
        "      for i in X:\n",
        "        i[1]=d1[j]\n",
        "        i[2]=d2[j]\n",
        "        j+=1\n",
        "        if j==len(d1):\n",
        "          break\n",
        "      d=np.array([0,1,1,0])\n",
        "    else:\n",
        "      print(\"NOR function\")\n",
        "      d = np.array([1,0,0,0])\n",
        "    W = train_perceptron(X,d)\n",
        "    print(\"Perceptron Algorithm:\")\n",
        "    j=0\n",
        "    for i in X:\n",
        "      print(X1[j][1],\" \",X1[j][2],\" \",np.dot(W,i)>0)\n",
        "      j+=1\n",
        "    print()\n",
        "    print(\"Adjusted training set - Perceptron Algorithm\")\n",
        "    W = adjust_train_perceptron(X,d)\n",
        "    j=0\n",
        "    for i in X:\n",
        "      print(X1[j][1],\" \",X1[j][2],\" \",np.dot(W,i)>0)\n",
        "      j+=1\n",
        "    print()\n",
        "    print(\"Perceptron Algorithm - learning rate:\")\n",
        "    W = train_perceptron_learn(X,d)\n",
        "    j=0\n",
        "    for i in X:\n",
        "      print(X1[j][1],\" \",X1[j][2],\" \",np.dot(W,i)>0)\n",
        "      j+=1\n",
        "    print()\n",
        "    print(\"Adjusted training set Perceptron Algorithm - learning rate:\")\n",
        "    W = train_perceptron_learn(X,d)\n",
        "    j=0\n",
        "    for i in X:\n",
        "      print(X1[j][1],\" \",X1[j][2],\" \",np.dot(W,i)>0)\n",
        "      j+=1\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X7aOipFHQm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2479c24-3e41-4036-b11d-3678b17c799d"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. AND function\n",
            "2. OR function\n",
            "3. NAND function\n",
            "4. NOR function\n",
            "5. IMPLIES function\n",
            "6. NOT IMPLIES function\n",
            "7. XOR function\n",
            "8. XNOR function8\n",
            "\n",
            "XNOR function as a network\n",
            "Perceptron Algorithm:\n",
            "0   0   True\n",
            "0   1   False\n",
            "1   0   False\n",
            "1   1   True\n",
            "\n",
            "Adjusted training set - Perceptron Algorithm\n",
            "0   0   True\n",
            "0   1   False\n",
            "1   0   False\n",
            "1   1   True\n",
            "\n",
            "Perceptron Algorithm - learning rate:\n",
            "Choosen learning rate:  0.36952025958502366\n",
            "0   0   True\n",
            "0   1   False\n",
            "1   0   False\n",
            "1   1   True\n",
            "\n",
            "Adjusted training set Perceptron Algorithm - learning rate:\n",
            "Choosen learning rate:  0.23173308498769574\n",
            "0   0   True\n",
            "0   1   False\n",
            "1   0   False\n",
            "1   1   True\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wen7xW74vG_"
      },
      "source": [
        "## Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idBnLaty4uTu"
      },
      "source": [
        "from vpython import *\n",
        "\n",
        "scene = canvas()\n",
        "\n",
        "#w1 = [[5,5,-5],[-5,5,5]]\n",
        "#b1 = [0,-15,0]\n",
        "#w2 = [[5],[ -15],[5]]\n",
        "#b2 = [-5]\n",
        "def train_1_epoch(W,X,x0,x1):\n",
        "  x0 = x0.tolist()\n",
        "  x1 = x1.tolist()\n",
        "  for i in X:\n",
        "    i = i.tolist()\n",
        "    \t# print(W,i)\n",
        "    \t# print(np.dot(W,i))\n",
        "    if i in x0:\n",
        "        if np.dot(W,i)>=0:\n",
        "            \t# print(\"1a\")\n",
        "            W = W-i\n",
        "    elif i in x1:\n",
        "        if np.dot(W,i)<0:\n",
        "            \t# print(\"2a\")\n",
        "            W = W+i\n",
        "  return W\n",
        "\n",
        "def make_nn(k, input_dim):\n",
        "\tglobal hidden_layers, bias_arrows, bias_text, weight_arrows, weight_text\n",
        "\thidden_layers = []\n",
        "\thidden_layers.append([])\n",
        "\tm  = int(len(k)/2)\n",
        "\tx_pos = 0\n",
        "\tcolors = [color.green, color.yellow, color.blue, color.red, color.orange]\n",
        "  for i in range(input_dim):\n",
        "    y_pos = -input_dim + 1 + 2*i\n",
        "    hidden_layers[0].append(cylinder(pos=vector(x_pos,y_pos,0), axis= vector(0,0,0.1), color = colors[0], radius = 0.5, opacity=0))\n",
        "   \t \n",
        "\tfor j in range(m):\n",
        "    \tx_pos = 2*(j+1)\n",
        "    \thidden_layers.append([])\n",
        "    \tn = len(k[j*2+1])\n",
        "    \tstart_y = -n + 1\n",
        "    \tfor i in range(n):\n",
        "        \ty_pos = start_y+2*i\n",
        "        \thidden_layers[j+1].append(cylinder(pos=vector(x_pos,y_pos,0), axis= vector(0,0,0.1), color = colors[j+1], radius = 0.5))\n",
        "    \n",
        "\tinput_arrows = []\n",
        "\tinput_text = []\n",
        "\tfor i in range(len(hidden_layers[0])):\n",
        "    \tstart_pos = hidden_layers[0][i].pos+ vector(-0.75,0,0)\n",
        "    \ttext_pos = start_pos + vector(0.5,0,0)\n",
        "    \taxi = vector(0.25,0,0)\n",
        "    \tinput_arrows.append(arrow(pos=start_pos,axis = axi, shaftwidth = 0.025, opacity=0))\n",
        "    \tinput_text.append(text(text = 'x'+ str(i+1), pos = text_pos, align = 'center', height = 0.2, color = color.red))\n",
        "    \n",
        "\tstart_pos = hidden_layers[-1][0].pos + vector(0.5,0,0)\n",
        "\ttext_pos = start_pos + vector(0.25,0,0)\n",
        "\taxi = vector(0.25,0,0)\n",
        "\toutput_arrow = arrow(pos=start_pos,axis = axi, shaftwidth = 0.025)\n",
        "\toutput_text = text(text = 'S', pos = text_pos, align = 'center', height = 0.2, color = color.red)\n",
        "    \n",
        "\tbias_arrows = []\n",
        "\tbias_text = []\n",
        "\tfor i in range(1,m+1):\n",
        "    \tbias_arrows.append([])\n",
        "    \tbias_text.append([])\n",
        "    \tn = len(hidden_layers[i])\n",
        "    \tfor j in range(n):\n",
        "        \tstart_pos = hidden_layers[i][j].pos + vector(0,1.25,0)\n",
        "        \ttext_pos = start_pos\n",
        "        \taxi = vector(0,-0.75,0)\n",
        "        \twt = abs(k[2*i-1][j])\n",
        "        \tbias_arrows[i-1].append(arrow(pos=start_pos,axis = axi, shaftwidth = 0.1*wt, opacity=0.5))\n",
        "        \tbias_text[i-1].append(text(text=str(round(k[2*i-1][j],3)), align='center', pos = text_pos, height = 0.2, color = color.red))\n",
        "       \t \n",
        "\tweight_arrows = []\n",
        "\tweight_text = []\n",
        "\tfor i in range(1,m+1):\n",
        "    \tweight_arrows.append([])\n",
        "    \tweight_text.append([])\n",
        "    \tif i==m:\n",
        "        \tn2 = 1\n",
        "    \telse:\n",
        "        \tn2 = len(k[i*2])\n",
        "    \tn1 = len(k[i*2-2])\n",
        "    \tprint(n1,n2)\n",
        "    \tfor i1 in range(n1):\n",
        "        \tweight_arrows[i-1].append([])\n",
        "        \tweight_text[i-1].append([])\n",
        "        \tfor i2 in range(n2):\n",
        "            \tprint(k[i*2-2][i1][i2])\n",
        "            \tstart_pos = hidden_layers[i-1][i1].pos\n",
        "            \tend_pos = hidden_layers[i][i2].pos\n",
        "            \taxi = end_pos-start_pos\n",
        "           \t \n",
        "            \tmid_pos = (4*start_pos + 3*end_pos)/7\n",
        "            \ttext_pos = mid_pos\n",
        "            \ttext_weight = str(round(k[i*2-2][i1][i2],3))\n",
        "            \twt = abs(k[i*2-2][i1][i2])\n",
        "           \t \n",
        "            \tweight_arrows[i-1][i1].append(arrow(pos=start_pos,axis = axi, shaftwidth = 0.1*wt, opacity=0.5))\n",
        "            \tweight_text[i-1][i1].append(text(text=text_weight, pos = text_pos, align = 'center', height = 0.2, color = color.red))\n",
        " \n",
        "def change_weights(k):\n",
        "\tglobal hidden_layers, bias_arrows, bias_text, weight_arrows, weight_text\n",
        "\tm  = int(len(k)/2)\n",
        "\tfor i in range(1,m+1):\n",
        "    \tn = len(hidden_layers[i])\n",
        "    \tfor j in range(n):\n",
        "        \tstart_pos = hidden_layers[i][j].pos + vector(0,0.75,0)\n",
        "        \ttext_pos = start_pos\n",
        "        \taxi = vector(0,-0.25,0)\n",
        "        \twt = abs(k[2*i-1][j])\n",
        "       \t \n",
        "        \tbias_arrows[i-1][j].shaftwidth = 0.05*wt\n",
        "        \tbias_text[i-1][j].visible = False\n",
        "        \tbias_text[i-1][j] = text(text=str(round(k[2*i-1][j],3)), align='center', pos = text_pos, height = 0.2, color = color.red)\n",
        "       \t \n",
        "\tfor i in range(1,m+1):\n",
        "    \tif i==m:\n",
        "        \tn2 = 1\n",
        "    \telse:\n",
        "        \tn2 = len(k[i*2])\n",
        "    \tn1 = len(k[i*2-2])\n",
        "    \tfor i1 in range(n1):\n",
        "        \tfor i2 in range(n2):\n",
        "            \tstart_pos = hidden_layers[i-1][i1].pos\n",
        "            \tend_pos = hidden_layers[i][i2].pos\n",
        "            \taxi = end_pos-start_pos\n",
        "           \t \n",
        "            \tmid_pos = (4*start_pos + 3*end_pos)/7\n",
        "            \ttext_pos = mid_pos\n",
        "            \ttext_weight = str(round(k[i*2-2][i1][i2],3))\n",
        "            \twt = abs(k[i*2-2][i1][i2])\n",
        "           \t \n",
        "            \tweight_arrows[i-1][i1][j].shaftwidth = 0.05*wt\n",
        "            \tweight_text[i-1][i1][j].visible = False\n",
        "            \tweight_text[i-1][i1][j] = text(text=text_weight, pos = text_pos, align = 'center', height = 0.2, color = color.red)\n",
        " \n",
        "def train_perceptron(X,d):\n",
        "\tglobal scene\n",
        "\tscene.center = vector(2,0,0)\n",
        "\tk = [[[0],[0]],[0]]\n",
        "\tinput_dim = 2\n",
        "\tmake_nn(k, input_dim)\n",
        "\t#this works for the overall Logical gate\n",
        "\t#Splitting positive and negative classes\n",
        "\tx1 = []\n",
        "\tx0 = []\n",
        "\tfor i in range(len(d)):\n",
        "    \tif(d[i] == 1):\n",
        "        \tx1.append(X[i])\n",
        "    \telse:\n",
        "        \tx0.append(X[i])\n",
        "\tx1 = np.array(x1)\n",
        "\tx0 = np.array(x0)\n",
        "\t#print(x0,\"\\n\\n\",x1)\n",
        "    \n",
        "\t#Assuming random weights\n",
        "\tW = np.random.rand(3)\n",
        "    \n",
        "\tflag = True\n",
        "\tmax_epochs = 100\n",
        "\n",
        "\t#training until convergence or 100 epochs\n",
        "\twhile flag == True and max_epochs>0:\n",
        "    \tmax_epochs -= 1\n",
        "    \tWi = W\n",
        "    \tWi1 = train_1_epoch(W,X,x0,x1)\n",
        "    \tdelta_W = Wi==Wi1\n",
        "    \t# To check if Wi1 == Wi\n",
        "    \tif delta_W.all():\n",
        "        \t#print(True)\n",
        "        \tbreak\n",
        "    \telse:\n",
        "        \tW = Wi1\n",
        "#         \tscene.delete()\n",
        "#         \tscene = canvas()\n",
        "        \tscene.center = vector(2,0,0)\n",
        "#        \tw1 = [[5,-5],[5,-5]]\n",
        "#        \tb1 = [-5,5]\n",
        "#        \tw2 = [[5],[5]]\n",
        "#        \tb2 = [-5]\n",
        "#        \tk = [w1, b1,w2, b2]\n",
        "        \tk = [[[W[1]],[W[2]]],[W[0]]]\n",
        "        \tprint(k)\n",
        "        \tinput_dim = 2\n",
        "        \tchange_weights(k)\n",
        "        \tsleep(2)    \n",
        "\t# print(W)\n",
        "\treturn W\n",
        "\n",
        "X = np.array([\n",
        "           \t[1,0,0],\n",
        "           \t[1,0,1],\n",
        "           \t[1,1,0],\n",
        "           \t[1,1,1]\n",
        "        \t])\n",
        "d = np.array([0,0,0,1])\n",
        "train_perceptron(X,d)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
